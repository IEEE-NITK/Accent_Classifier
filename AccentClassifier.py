# -*- coding: utf-8 -*-
"""AccentClassifierwithOversampling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tb7HaFjGlVmcn3TbogUg8f0EtOModPHV

## MOUNTING GOOGLE DRIVE
"""

from google.colab import drive
drive.mount('/content/drive')

import os
audio_folder = "/content/drive/MyDrive/ASP IEEE project/allaudio"
# List audio files
files = os.listdir(audio_folder)
print("Audio Files in Folder:", files)

"""## VISUALISING THE AUDIO"""

import librosa
import librosa.display
import numpy as np
import matplotlib.pyplot as plt
#from pydub import AudioSegment

# Select an audio file (replace with your actual file name)
file_name = "english73.wav"
audio_path = os.path.join(audio_folder, file_name)

# Load audio using Librosa
y, sr = librosa.load(audio_path, sr=None)

# Display waveform
plt.figure(figsize=(10, 4))
librosa.display.waveshow(y, sr=sr)
plt.title('Waveform of Audio')
plt.show()

"""## MFCC (Mel-Frequency Cepstral Coefficients) —
Speech DNA
A compressed version of Mel Spectrogram that focuses on how humans hear.

Inspired by how our ears work!

MFCC simplifies audio by picking out the most important frequencies that help recognize speech.

It turns sound into about 13–40 values per frame—kind of like summarizing the speech shape.

Think of it like:
"What kind of voice is this?” — tone, clarity, accent.

## Parselmouth
While tools like MFCCs or Mel spectrograms focus on general sound patterns, Parselmouth is great for extracting linguistic features such as:
Pitch ,Intensity ,Formants (Vocal tract resonances, useful for vowel sounds)Frequency bands that shape vowels.

These features are super helpful for analyzing accents, speaker identity, or emotional tone.

## MELSPECTOGRAMS
You’ll see a colorful 2D image:

X-axis → time

Y-axis → frequency :in Mel scale

Color → intensity :how strong each frequency is
It shows how much of each frequency is present in the audio over time.

Think of it like:
“What notes were played, and how loudly?”
"""

import librosa
import librosa.display
import numpy as np
import matplotlib.pyplot as plt
import os

# Set your audio file path
audio_folder = "/content/drive/MyDrive/ASP IEEE project/allaudio"  # or wherever your files are stored
file_name = "english73.wav"
audio_path = os.path.join(audio_folder, file_name)

# Load audio
y, sr = librosa.load(audio_path, sr=None)

# Compute Mel Spectrogram
S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)
S_dB = librosa.power_to_db(S, ref=np.max)  # Convert to decibels

# Plot the Mel Spectrogram
plt.figure(figsize=(10, 4))
librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel')
plt.colorbar(format='%+2.0f dB')
plt.title('Mel Spectrogram')
plt.tight_layout()
plt.show()

#define your folder structure
data_dir = "/content/drive/MyDrive/ASP IEEE project/allaudio"

"""## Extracting Male Speaker Audio Metadata"""

import pandas as pd
import re

# Load the CSV file directly (update the file path accordingly)
csv_file = "/content/drive/MyDrive/bio_metadata.csv"  # Change this to your actual CSV file path
df = pd.read_csv(csv_file)

# Check if required columns exist
required_columns = ['language_num', 'sex', 'native_language']
missing_columns = [col for col in required_columns if col not in df.columns]

if missing_columns:
    print(f"Missing columns in CSV: {missing_columns}")
else:
    # Filter rows where sex is "male"
    df_male = df[df['sex'].str.lower() == 'male']

    # Keep only 'language_num' and 'native_language'
    df_male = df_male[['language_num', 'native_language']]

    # Add ".wav" extension to language_num column
    df_male['language_num'] = df_male['language_num'] + ".wav"

    # Remove all brackets [] and text inside them, as well as (cmn)
    df_male['native_language'] = df_male['native_language'].apply(lambda x: re.sub(r'\[.*?\]|\(.*?\)', '', x).strip())

    # Save to a new CSV file in the same directory as the original file
    output_file = csv_file.replace(".csv", "_male_audio.csv")  # Creates a new file with '_male_audio' suffix
    df_male.to_csv(output_file, index=False)

    print(f"Filtered data saved to {output_file}")

import numpy as np
import pandas as pd
# Path to your CSV file
csv_path = "/content/drive/MyDrive/ASP IEEE project/male_data.csv"

# Load CSV
df = pd.read_csv(csv_path)
print(df)  # Check first few rows

"""## Identifying Missing Files with Checkpoints"""

import os
import pandas as pd
from tqdm import tqdm

# Paths
csv_path = '/content/drive/MyDrive/ASP IEEE project/male_data.csv'
audio_dir = "/content/drive/MyDrive/ASP IEEE project/allaudio"
checkpoint_path = '/content/drive/MyDrive/ASP IEEE project/missing_files.txt'

# Load CSV
df = pd.read_csv(csv_path)

# Ensure CSV has the correct column
if "language_num" not in df.columns:
    raise ValueError("CSV must contain a 'language_num' column with filenames.")

# List all audio files in the directory
audio_files = set(os.listdir(audio_dir))

# Load previous checkpoint if exists
missing_files = []
if os.path.exists(checkpoint_path):
    with open(checkpoint_path, 'r') as f:
        missing_files = [line.strip() for line in f.readlines()]
    print(f"Resuming from checkpoint: {len(missing_files)} missing files already found.")

# Identify missing files
print("Checking for missing audio files...")

with open(checkpoint_path, 'a') as f:
    for file in tqdm(df["language_num"], desc="Processing files"):
        if file not in audio_files and file not in missing_files:
            print(f"Missing: {file}")
            missing_files.append(file)
            f.write(file + "\n")

# Final status
if missing_files:
    print(f"\nTotal Missing Files: {len(missing_files)} (saved to {checkpoint_path})")
else:
    print("\nAll audio files are present!")

print(df["native_language"].value_counts())

"""## Wav2Vec2 Feature Extraction from Male Audio Data

A self-supervised speech model by Facebook AI.Pretrained on massive unlabeled audio datasets.Learns contextual representations of raw audio — no need for handcrafted features.


No need for MFCCs, spectrograms, or manual preprocessing.
Captures subtle acoustic patterns like pitch, tone, and rhythm — crucial for accent recognition.
Robust to background noise and speaker variations.

Think of this as the model learning "sound patterns" like tones, pitch, energy, etc.These layers learn the context and relationships between different parts of the speech.

What Are Transformers?
Transformers are a type of deep learning model designed to handle sequential data, like:
Sentences in language (text)
Audio waveforms
They were originally invented for Natural Language Processing (NLP) but are now used in audio (like Wav2Vec2), vision, code, and more!Traditional models like RNNs/LSTMs read one word/sound at a time in order.
But transformers read the entire input at once, allowing parallel processing — way faster and more powerful!
"""

import os
import torch
import librosa
import numpy as np
import pandas as pd
from tqdm import tqdm
from transformers import Wav2Vec2Processor, Wav2Vec2Model

# Load the CSV file (update with actual path)
csv_path = '/content/drive/MyDrive/ASP IEEE project/male_data.csv'
df = pd.read_csv(csv_path)

# Define paths
audio_dir = "/content/drive/MyDrive/ASP IEEE project/allaudio"

# Load Wav2Vec2 model and processor
model_name = "facebook/wav2vec2-base-960h"  # You can try larger models for better performance
processor = Wav2Vec2Processor.from_pretrained(model_name)
model = Wav2Vec2Model.from_pretrained(model_name)
model.eval()  # Set model to evaluation mode

# Prepare storage for features and labels
data_features = []
labels = []

# Ensure CSV has required columns
if "language_num" not in df.columns or "native_language" not in df.columns:
    raise ValueError("CSV must contain 'language_num' and 'native_language' columns.")

# Convert labels to numerical format
class_names = sorted(df["native_language"].unique())  # Get unique labels
label_to_index = {label: i for i, label in enumerate(class_names)}

print("Processing audio files...")

for index, row in tqdm(df.iterrows(), total=len(df)):
    file_name = row["language_num"]
    file_path = os.path.join(audio_dir, file_name)

    if not os.path.exists(file_path):
        print(f"Missing file: {file_name}")
        continue  # Skip missing files

    try:
        # Load audio file
        audio_data, sample_rate = librosa.load(file_path, sr=16000)  # Wav2Vec expects 16kHz audio

        # Convert to tensor and process
        inputs = processor(audio_data, sampling_rate=16000, return_tensors="pt", padding=True)
        with torch.no_grad():
            outputs = model(**inputs)

        # Extract last hidden state embeddings
        embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()

        # Append data and labels
        data_features.append(embeddings)
        labels.append(label_to_index[row["native_language"]])

    except Exception as e:
        print(f"Error processing {file_name}: {e}")

# Convert lists to NumPy arrays
data_features = np.array(data_features)
labels = np.array(labels)

# Save processed data for future training
# Define output paths
target_dir = "/content/drive/MyDrive/ASP IEEE project/Wav2vec features"
os.makedirs(target_dir, exist_ok=True)  # Create directory if it doesn't exist

np.save(os.path.join(target_dir, "wav2vec_features.npy"), data_features)
np.save(os.path.join(target_dir, "wav2vec_labels.npy"), labels)

print("Feature extraction complete!")
print("Data shape:", data_features.shape)
print("Labels shape:", labels.shape)

"""## OVERSAMPLING AS THERE ARE FEW SAMPLES OF MANDARIN

SMOTE: Synthetic Minority Over-sampling Technique
It’s a data augmentation technique used to balance imbalanced datasets.
SMOTE helps by creating synthetic examples of the minority class — not just copying them.
Interpolates between the sample and its neighbors to create new, slightly different synthetic data points.
It’s like saying: “Let’s generate new voices that are similar to existing Mandarin samples but not exact duplicates.”
"""

import numpy as np
from imblearn.over_sampling import SMOTE
from collections import Counter

# Ensure lists are converted to NumPy arrays
X = np.array(data_features)
y = np.array(labels)

# Ensure labels are integers and 1D

print("Shape of features (X):", X.shape)  # Confirm shape
print("Shape of labels (y):", y.shape)    # Confirm shape

# Check class distribution before SMOTE
#print("Before SMOTE class distribution:", Counter(y))

# Apply SMOTE to balance the classes
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

#print("After SMOTE class distribution:", Counter(y_resampled))

# Save the balanced dataset
# Save processed data for future training
# Define output paths
target_dir = "/content/drive/MyDrive/ASP IEEE project/Wav2vec features"
os.makedirs(target_dir, exist_ok=True)  # Create directory if it doesn't exist

# Correctly specify file paths
features_path = os.path.join(target_dir, "wav2vec_features_balanced.npy")
labels_path = os.path.join(target_dir, "wav2vec_labels_balanced.npy")

# Save the files
np.save(features_path, X_resampled)
np.save(labels_path, y_resampled)

print("Balanced dataset saved successfully!")
print(y_resampled.shape)

print("Type of X:", type(data_features))
print("Shape of X:", np.array(data_features).shape)

print("Type of y:", type(labels))
print("Shape of y:", np.array(labels).shape)

print("Unique labels in y:", np.unique(labels))# Summary of processing
print("\n=== Processing Summary ===")
print(f"Total files in CSV: {len(df)}")
print(f"Successfully processed: {len(X_resampled)}")

# Print data shape
print("\n=== Data Shapes ===")
print("DATA Features Shape:", X_resampled.shape)  # (num_samples, n_mfcc, max_pad_length)
#print("Labels Shape:", labels.shape)

X_resampled= X_resampled.reshape(X_resampled.shape[0], -1)

from sklearn.model_selection import train_test_split

# Split the dataset (e.g., 80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)

print(f"Train shape: {X_train.shape}, {y_train.shape}")
print(f"Test shape: {X_test.shape}, {y_test.shape}")

"""## CREATING THE MODEL"""

from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout

model2=Sequential()
###first layer
model2.add(Dense(100,input_shape=(768,)))
model2.add(Activation('relu'))
model2.add(Dropout(0.5))
###second layer
model2.add(Dense(200))
model2.add(Activation('relu'))
model2.add(Dropout(0.5))
###third layer
model2.add(Dense(100))
model2.add(Activation('relu'))
model2.add(Dropout(0.5))
###fourth layer
model2.add(Dense(100))
model2.add(Activation('relu'))
model2.add(Dropout(0.5))
###final layer
model2.add(Dense(3))
model2.add(Activation('softmax'))

model2.summary()

"""## TRAINING THE MODEL

Dropout:Dropout is a regularization technique used to prevent overfitting.During training, random neurons in your network are "turned off" (i.e., dropped) in each iteration.
This forces the network to not rely too heavily on any one feature, and instead learn robust, generalized patterns.

Epochs: One epoch means the model has seen every training sample once.But too many epochs? It might overfit (memorize the training data).
"""

import numpy as np
from keras import backend as K
import tensorflow as tf

# Ensure consistent data types (convert to float32)
X_resampled = X_resampled.astype('float32')
y_resampled = y_resampled.astype('float32')
X_test = X_test.astype('float32')
y_test = y_test.astype('float32')

# Clear any previous TensorFlow session (prevents graph errors)
K.clear_session()
tf.compat.v1.reset_default_graph()

# Check data shapes and types
print(f"X_resampled shape: {X_resampled.shape}, dtype: {X_resampled.dtype}")
print(f"y_resampled shape: {y_resampled.shape}, dtype: {y_resampled.dtype}")
print(f"X_test shape: {X_test.shape}, dtype: {X_test.dtype}")
print(f"y_test shape: {y_test.shape}, dtype: {y_test.dtype}")

# Ensure correct loss function based on label shape
loss_function = "categorical_crossentropy" if y_resampled.ndim == 2 else "sparse_categorical_crossentropy"

# Compile the model
model2.compile(optimizer='adam', loss=loss_function, metrics=['accuracy'])

# Train the model with oversampled data
num_epochs = 100
num_batch_size = 32

history = model2.fit(
    X_resampled, y_resampled,
    batch_size=num_batch_size,
    epochs=num_epochs,
    validation_data=(X_test, y_test),
    verbose=1
)

"""## SAVING THE MODEL"""

model2.save('my_model_oversampled2.h5')
new_model = tf.keras.models.load_model('my_model_oversampled2.h5')
new_model.summary()

from google.colab import files
files.download('my_model_oversampled2.h5')

import tensorflow as tf

"""## LOADING THE PRETRAINED MODEL"""

# Load your pretrained accent classifier model
model = tf.keras.models.load_model('/content/my_model_oversampled2.h5')
print("Pretrained model loaded successfully!")

import numpy as np
from tensorflow.keras.models import load_model
from sklearn.metrics import accuracy_score

# Load the trained model

# Load your data
X_test = np.load('/content/drive/MyDrive/ASP IEEE project/Wav2vec features/wav2vec_features_balanced.npy')  # features
y_test = np.load('/content/drive/MyDrive/ASP IEEE project/Wav2vec features/wav2vec_labels_balanced.npy')  # labels (should be one-hot encoded if model uses categorical_crossentropy)

# Predict class probabilities
y_pred_probs = model.predict(X_test)

# Get predicted class labels
y_pred = np.argmax(y_pred_probs, axis=1)

# Compare directly to label vector
accuracy = np.mean(y_pred == y_test)
print(f"Accuracy: {accuracy * 100:.2f}%")

"""## CONFUSION MATRIX"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
from tensorflow.keras.models import load_model
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

model = load_model("/content/my_model_oversampled2.h5")  # Update path as needed

# Step 2: Load your test data (update path as needed)
X_test = np.load("/content/drive/MyDrive/ASP IEEE project/Wav2vec features/wav2vec_features_balanced.npy")
y_test = np.load("/content/drive/MyDrive/ASP IEEE project/Wav2vec features/wav2vec_labels_balanced.npy")

# Your test labels are already integers
y_true = y_test  # No need for argmax

# Get predicted class from probabilities
y_pred = np.argmax(y_pred_probs, axis=1)

# Print precision, recall, f1-score
print(classification_report(y_true, y_pred, target_names=["Arabic", "English", "Mandarin"]))
# Step 1: Load your trained model
# Create the confusion matrix
conf_matrix = confusion_matrix(y_true, y_pred)

# Plot
plt.figure(figsize=(6, 5))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues",
            xticklabels=["Arabic", "English", "Mandarin"],  # Change if needed
            yticklabels=["Arabic", "English", "Mandarin"])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

"""## ROC Curve:Receiver Operating Characteristic curve
It helps you see how confidently your model can distinguish one accent from the others.
A better model hugs the top-left corner of the plot.
The area under the ROC curve gives you a single number to compare:
AUC(area under curve) = 1.0 → Perfect classifier
AUC = 0.5 → Random guessing
"""

from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
from itertools import cycle
import numpy as np

# Labels and predicted probabilities
# y_test: (n_samples,) with integer class labels
# y_pred_probs: (n_samples, n_classes) with predicted probabilities

# Get number of classes
n_classes = y_pred_probs.shape[1]

# One-hot encode y_test
y_test_bin = label_binarize(y_test, classes=list(range(n_classes)))

# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_probs[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Plot all ROC curves
colors = cycle(['blue', 'red', 'green', 'purple'])
plt.figure(figsize=(8, 6))
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2,
             label=f"Class {i} (AUC = {roc_auc[i]:.2f})")

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - Accent Classifier")
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred, target_names=class_names))

from sklearn.metrics import confusion_matrix
import seaborn as sns

conf_matrix = confusion_matrix(y_test, y_pred, normalize='true')
sns.heatmap(conf_matrix, annot=True, cmap='Blues', xticklabels=class_names, yticklabels=class_names)

"""## t-SNE
stands for t-distributed Stochastic Neighbor Embedding.
It’s a dimensionality reduction technique used to visualize high-dimensional data.
Wav2Vec2 model creates 768-dimensional feature vectors per audio file.WHich is too big to plot or understand visually.
t-SNE shrinks it down to 2D or 3D while keeping similar accents close together and different ones far apart.
"""

from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

X_embedded = TSNE(n_components=2).fit_transform(X_resampled)
plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y_resampled, cmap='viridis')
plt.title("t-SNE Visualization of Accent Features")
plt.colorbar()
plt.show()

!pip install gradio

import gradio as gr
import numpy as np
import torch
import librosa
from tensorflow.keras.models import load_model
from transformers import Wav2Vec2Processor, Wav2Vec2Model

# Load the pretrained Keras model (.h5)
accent_model = load_model("/content/my_model_oversampled2.h5")

# Load Wav2Vec processor and model
processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")
wav2vec_model = Wav2Vec2Model.from_pretrained("facebook/wav2vec2-base-960h")

# Label mapping (update with your class labels)
accent_labels = ['Arabic', 'English', 'Mandarin']

# Preprocess audio using Wav2Vec pipeline
def preprocess_audio_wav2vec(audio_path):
    # Load and resample audio to 16kHz
    audio_data, sample_rate = librosa.load(audio_path, sr=16000)

    # Convert audio to tensor (required for Wav2Vec)
    inputs = processor(audio_data, sampling_rate=16000, return_tensors="pt", padding=True)

    # Extract Wav2Vec embeddings
    with torch.no_grad():
        outputs = wav2vec_model(**inputs)

    # Get mean-pooled embeddings (shape: [1, 768])
    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()

    # Ensure shape matches the model input
    return np.expand_dims(embeddings, axis=0)

# Prediction function for Gradio
def classify_accent(audio):
    # Save the audio file temporarily
    audio_path = audio

    try:
        # Preprocess audio to get Wav2Vec embeddings
        input_data = preprocess_audio_wav2vec(audio_path)

        # Predict using the accent model
        prediction = accent_model.predict(input_data)

        # Determine the predicted accent and confidence
        predicted_accent = accent_labels[np.argmax(prediction)]
        confidence = np.max(prediction) * 100

        return f" Predicted Accent: {predicted_accent} ({confidence:.2f}% confidence)"

    except Exception as e:
        return f"Error processing audio: {str(e)}"

# Gradio interface
interface = gr.Interface(
    fn=classify_accent,
    inputs=gr.Audio(type="filepath"),
    outputs="text",
    title="Accent Classifier",
    description="Upload an audio file (.wav) to classify its accent [English,Arabic,Mandarin]"
)

interface.launch()